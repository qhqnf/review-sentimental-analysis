{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = os.getcwd()\n",
    "file_path = cur_path + \"\\Documents\\\\technical-assignment-2020-0823\"\n",
    "train_data_path = file_path + '\\\\test_data'\n",
    "\n",
    "valid_data_path = file_path + '\\\\test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train data\n",
    "test_data = pd.read_csv(valid_data_path, sep='\\n', names=[\"review\"], header=None, skip_blank_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                    review\n",
       "0                               진짜 가만히 있어도 눈물이 나옵니다ㅠㅠㅠㅠㅠㅠ\n",
       "1       영화를 분석하며 보는 불쌍한 사람이 되지 마세요 아이 키우는 아빠로서 많은 눈물을 ...\n",
       "2                   정말 다시보고싶은영화   적극추천합니다배우들연기 짱  기대이상입니다\n",
       "3                                  대박 ㅠㅠ웃음에서감동까지 최고의영화입니다\n",
       "4                        감동있지만 개인적으로 지적장애로 유발된 웃음은 싫어하는터라\n",
       "...                                                   ...\n",
       "399995                   같은여자인데 엘리제한테반햇어요 노래도좋고 힐링되는영화였어요\n",
       "399996           믿음을 부서트리기 위한 악마의 장치 브로큰 써클 욥기를 다시금 되새겨본다\n",
       "399997                             나도 이렇게 멋진 사랑을 하고 싶다아아앙\n",
       "399998                                         삐료 정말화가난다 \n",
       "399999                      맨날 울고불고 꼭 누구 하나 죽고 뻔한 구닥다리 방화\n",
       "\n",
       "[400000 rows x 1 columns]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 샘플의 개수 : 400000\n"
     ]
    }
   ],
   "source": [
    "# replace all other string to blank except korean\n",
    "test_data['review'] = test_data['review'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "#test_data['review'].replace('', '굳', inplace=True)\n",
    "test_data = test_data.fillna('굳')\n",
    "#test_data = test_data.dropna(how = 'any')\n",
    "\n",
    "\n",
    "print('전처리 후 샘플의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 객체 계속 쓰기\n",
    "X_test = []\n",
    "for sentence in test_data['review']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_tokenized_data.p', 'wb') as file:\n",
    "    pickle.dump(X_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84222\n",
      "84222\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 객체 불러오기\n",
    "with open('tokenized_data1.p', 'rb') as file:\n",
    "    X_train = pickle.load(file)\n",
    "    \n",
    "with open('data1_label.p', 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "    \n",
    "with open('test_tokenized_data.p', 'rb') as file:\n",
    "    X_test = pickle.load(file)\n",
    "    \n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 29531\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 17187\n",
      "단어 집합에서 희귀 단어의 비율: 58.19985777657377\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 2.3858058875623716\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 12346\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = 30)\n",
    "X_test = pad_sequences(X_test, maxlen = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "y_test = []\n",
    "\n",
    "def sentiment_predict(new_sentence):\n",
    "    score = float(loaded_model.predict(new_sentence)) # 예측\n",
    "    y_test.append(math.ceil(score*10))\n",
    "\n",
    "for sentence in X_test:\n",
    "    sentiment_predict(np.array([sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = os.getcwd()\n",
    "file_path = cur_path + \"\\Documents\\\\technical-assignment-2020-0823\"\n",
    "submission_file_path = file_path + '\\\\submission.csv'\n",
    "\n",
    "submission = pd.DataFrame(y_test, columns=[\"prediction\"])\n",
    "submission.to_csv(submission_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0.00227015]]\n",
      "0.0022701455745846033\n",
      "99.77% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "\n",
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = 30) # 패딩\n",
    "    print(type(pad_new))\n",
    "    print(loaded_model.predict(pad_new))\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    print(score)\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n",
    "\n",
    "sentiment_predict('이 영화 핵노잼 ㅠㅠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = cur_path + \"\\Documents\\\\technical-assignment-2020-0823\"\n",
    "submission_file = file_path + '\\\\submission0911.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(submission_file, header=0, usecols=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             ID  prediction\n",
       "0            0          10\n",
       "1            1          10\n",
       "2            2          10\n",
       "3            3          10\n",
       "4            4           8\n",
       "...        ...         ...\n",
       "399995  399995          10\n",
       "399996  399996          10\n",
       "399997  399997          10\n",
       "399998  399998          10\n",
       "399999  399999           8\n",
       "\n",
       "[400000 rows x 2 columns]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(submission_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
